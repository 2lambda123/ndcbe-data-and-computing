{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUdg15uf30TU"
   },
   "source": [
    "# 01: Sampling\n",
    "\n",
    "CBE 20258. Numerical and Statistical Analysis. Spring 2020.\n",
    "\n",
    "&#169; University of Notre Dame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCqLvLrU30TW"
   },
   "source": [
    "## Lecture 15 Learning Objectives\n",
    "\n",
    "After studying this notebook and your notes, completing the activities, asking questions in class, you should be able to:\n",
    "* Give at least three examples of how an engineer would use statistics\n",
    "* Compute descriptive statistics for data using Pandas (Python package)\n",
    "* Explain what *statistical independence* means.\n",
    "* Interpret the elements of a covariance matrix\n",
    "* Explain why some data distributions have a very different median and mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(255,0,0,0.05) ; padding: 10px; border: 1px solid darkred;\"> \n",
    "<b>Note</b>: The home activities are optional and will count as extra credit. They are due Monday, March 16 @ 10am via Vocareum. You already completed many of these home activities in \"Class 3: Numpy, Matplotlib, and Pandas\". We recommend revisiting these as practice during spring break.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15a. Samlping\n",
    "\n",
    "**Further Reading**: ยง1.1 in Navidi (2015)\n",
    "\n",
    "### Basic Vocabulary and an Example\n",
    "\n",
    "Here is a central procedure that underpins the entire field of statistics:\n",
    "1. Collect a sample of data.\n",
    "2. Analyze the data to infer something about a population.\n",
    "\n",
    "We will make this concrete with an example. Imagine we are researching nutrition and health of college students. We want to know the average caloric intake (calories eaten per day) of all entire undergraduate population at Notre Dame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: Brainstorm at least two challenges with measuring the caloric intake of the entire student population.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reasons you just listed and more, we decide it is infeasible to accurately record everything that everyone on campus eats. Instead we decide to randomly select a sample of students and have them record a food diary. Of course, we'll have to train them on how to make accurate measurements, compensate them for their time, and take additional steps at estimate how accuracy of their journal entries.\n",
    "\n",
    "Using this example, let's introduce some vocaburaly:\n",
    "\n",
    "| Definition | Discussion for Our Example |\n",
    "| - | - |\n",
    "| **Population**: The entire collection of objects or outcomes about which information is sought. | We want to know about the eating habits of the entire **ND undergraduate student body**. |\n",
    "| **Sample**: A subset of the population, containing the objects or outcomes that are actually observed. | We ask a **small group of ND undergraduates** to keep a food journal. |\n",
    "| **Simple Random Sample**: A sample that is randomly choosen, where each collection of population items is equally likely to make up the sample. | We decide to randomly select $n$ undergraduate students by **drawing names of our a well-mixed hat**. |\n",
    "| **Convenience Sample**: A sample that is obtained is some convienent way, and not drawn from a well-defined random method. | We decide to **ask a few classes** on campus to keep food journals. |\n",
    "\n",
    "A fundamental limitations of convience samples is that they often introduce systematic biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: In two sentences, describe how asking only a few classes to keep food journals could lead us to misguided conclusions about the entire population of ND undergraduates. How would using a random sample help safegaurd against (some) of these biases?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so we randomly select 30 undergraduates and strap Go-Pros to their heads. We painstakingly analyze the videos to verify their food journals are 100% accurate. We then calculate the average caloric intake is 2023.2 kilocalories per day.\n",
    "\n",
    "Is this the true average of the population? In other words, if we somehow measured the caloric intake of the entire ND undergraduate student population, should we expect to the average to be exactly 2023.2 kcal/day?\n",
    "\n",
    "No. Why? Because our sample was randomly selected. Thus we expect to see **sampling variation**. In other words, if we collected another sample from 30 other randomly selected students, we expect a slightly different result. We will soon see how **probability theory** gives us a mathematical framework to quantify this variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptual versus Tangible Populations\n",
    "\n",
    "There are two types of populations:\n",
    "\n",
    "| Type of Population | Example |\n",
    "| - | - |\n",
    "| **Tangible Population**: A population that is finite. Often a tangigle population decreases by one after an object is sampled. | Population of students. |\n",
    "| **Conceptual Population**: A population that is infinite. | An engineer measures the concentration of a mixture five times. The population is the set of all possible outcomes of these five measurements. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: With a partner, for each of the following scenarions, i) define the population and ii) state whether it is conceptual or tangible.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A chemical process is run 15 times, and the yield is measured 15 times.\n",
    "2. In a clinical trial to test a new drung that is designed to lower cholesterol, 100 people with high cholesterol levels are recruited to try the new drug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Activity Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence\n",
    "\n",
    "The items in a sample are **independent** if knowing the values of some of the items does not help to predict the values of the others. Because tangigle populations are finite, samples taken without replacement are not strictly independent. For example, if we choose student \"John Doe\" to keep a food journal, we know that \"John Doe\" will not be chosen again (assuming everyone has a unique name and we are sampling without replacement). But we often treate simple random samples as independent if the sample contains less than 5% of the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15b. Working with Data using Panda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Sakai, you'll find `Datasets-All-Examples-Navidi.zip`. This file, which I downloaded from [the publisher](http://highered.mheducation.com/sites/0073401331/student_view0/data_sets.html), contains all of the data for the examples and tables in our textbook. We'll use many of these datasets to illustrate key concepts in class.\n",
    "\n",
    "Let's start with Tables 1.1 and 1.2 (pg. 21), which give **particulate matter (PM) emissions in g/gal** for 138 and 62 vehicles at low and high altitudes, respectively. Please take a moment to get out your textbook and glance at the tables. I'll wait.\n",
    "\n",
    "Now let's load the data into Python. In this class, we will use `Pandas`, which is a super popular and easy to use package/library/module for organizing and manipulating data. Here is a highly recommended [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) getting started tutorial.\n",
    "\n",
    "The code below reads in the first text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "low = pd.read_csv('table1-1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a Pandas **dataframe**, which is stored in the variable `low`. We can easily print its contents to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row (vehicle) is numbered 0, which is perhaps not a surprise. We see there are 138 rows in the dataset, which matches what we expect: data for 138 vehicles at low altitude.\n",
    "\n",
    "The output above is ugly. We can use the `.head()` and `.tail()` commands to look at only the first and last five entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "    <b>Optional Home Activity</b>: Load the high altitude data, which is stored in <tt>table1-2.csv</tt> into the Pandas dataframe <tt>high</tt>. Verify there are 62 rows. Use the head command to see the first few rows.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "high = pd.read_csv('table1-2.csv')\n",
    "\n",
    "print(\"Number of rows = \",len(high))\n",
    "\n",
    "high.head()\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "14b-high",
     "locked": true,
     "points": "0.1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "\n",
    "assert len(high) == 62, \"There should be 62 rows.\"\n",
    "\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14c. Summary Statistics\n",
    "\n",
    "**Further Reading**: ยง1.2 in Navidi (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Data: Numerical and Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As engineers, you will encounter both **numerical** (quantitative) and **categorical** (qualitative) data.\n",
    "\n",
    "Unfortunately, Example 1.8 from the textbook was not included in the data files on the publisher's website. But do not fear! We can recreate the table from a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all of the data from Example 1.8 in a dictionary\n",
    "# Notice the keys are the column names\n",
    "my_dict = {\"Specimen\":[1, 2, 3, 4, 5], \"Torque\":[165, 237, 222, 255, 194],\"Failure Location\":['Weld','Beam','Beam','Beam','Weld']}\n",
    "\n",
    "# Convert the dictionary into a Pandas dataframe\n",
    "my_df = pd.DataFrame(my_dict)\n",
    "\n",
    "# Print\n",
    "print(my_df)\n",
    "\n",
    "# Look at the first five entries\n",
    "my_df.head()\n",
    "\n",
    "# Profit???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was easy.\n",
    "\n",
    "In this example, we see that *Torque* is a numerical variable and *Failure Location* is a categorical variable.\n",
    "\n",
    "We will now learn about statistics to summarize key characteristics of samples. Let's start by focusing on numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Mean\n",
    "\n",
    "The sample mean is the average of the sample.\n",
    "\n",
    "Let $X_1$, $X_2$, ..., $X_n$ be the sample. The **sample mean** is\n",
    "\n",
    "$$\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$$\n",
    "\n",
    "Statisticians have many quirks, which make them the butts of many jokes. I would like to draw your attention to two:\n",
    "1. A capital variable, such as $X_i$, is often a **random variable**. More on this next class.\n",
    "2. Statisticians like to give variables decorations. Here $\\bar{~}$ (bar) means average. Later in the semester we'll see that $\\hat{~}$ (hat) means estimate.\n",
    "\n",
    "It is really easy to calculate the sample mean with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That output looks strange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(low.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. The command `low.mean()` does not return a floating point number. It instead returns a variable that is type `pandas.core.series.Series`. What if I just want the sample mean in a floating point number?\n",
    "\n",
    "In pandas, we can access a column using its name. Recall here are the first five elements in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set only has one column. But if we want to extract that column, we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get the numeric mean, we simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.PM.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "    <b>Optional Home Activity</b>: Calculate the sample mean for PM in the high altitude data set. Store your answer (a float) in the variable <tt>ans_14b_1</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "ans_14b_1 = high.PM.mean()\n",
    "print(ans_14b_1)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "14b-1",
     "locked": true,
     "points": "0.2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "import numpy as np\n",
    "assert np.abs(ans_14b_1 - 6.596) < 1E-2, \"Try again.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While mean reflects the average of a sample, variance and standard deviation measure the spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_1$, ..., $X_n$ be a sample. The **sample variance** is\n",
    "\n",
    "$$s^2 = \\frac{1}{n-1} \\sum_{i}^{n} (X_i - \\bar{X})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: If the measured quantitfy $X$ is velocity with units m/s, then what are the units of variance $s^2$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig into the idea that variance measures the spread of the data set. Consider a synthetic (i.e., made up, artificial) data sets with two columns, A and B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.DataFrame({\"A\":[0, 0, 5, 10, 10], \"B\":[4, 4, 5, 6, 6]})\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both columns have the same mean (average):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "        <b>Optional Home Activity</b>: Please take a minute to verify (on paper or in your head) that both data sets do in fact have a mean of 5.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, pandas just calcutaed the means for both columns in our synthetic data set.\n",
    "\n",
    "What about the variance of both columns? The variance formula,\n",
    "\n",
    "$$s^2 = \\frac{1}{n-1} \\sum_{i}^{n} (X_i - \\bar{X})^2$$\n",
    "\n",
    "sums the squared different between each datum (a.k.a., data point) and the mean. Thus we expect a data set with a larger spread to have a larger variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is in fact what we see with the variance calculation. Column A, with range 0 to 10, has a much larger variance than column B, which has a range of 4 to 6.\n",
    "\n",
    "Often we prefer to work with **sample standard derivation**, which is the square root of **sample variance**:\n",
    "\n",
    "$$s = \\sqrt{ \\frac{1}{n-1} \\sum_{i}^{n} (X_i - \\bar{X})^2}$$\n",
    "\n",
    "Notice that $s$ has the same units as $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why do the standard deviation and variance formulas divide by $n-1$ and not $n$? Short answer: often, we do not know the population standard deviation. (Recall, the sample was drawn from a population.) So we use $s$ as an estimate for the population standard deviation. This uses one degree of freedom, so we divide by $n-1$ instead of $n$. The $-1$ is because we want to estimate one parameter (population standard deviation). We will revist this idea a few times during the semester. A common exercise in a graduate level statistics course is to prove that dividing by $n-1$ makes $s^2$ an unbias estimate of population variance. Still curious? Check out this video: https://www.youtube.com/watch?v=D1hgiAla3KI\n",
    "\n",
    "If we really wanted to, we could change the degree of freedom from the default 1 to any number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide by n - 1 in variance and standard derviation formulae. This is the default\n",
    "print(\"variance\\n\",my_data.var(ddof=1),\"\\n\")\n",
    "print(\"standard deviation\\n\",my_data.std(ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide by n in variance and standard derviation formulae.\n",
    "print(\"variance\\n\",my_data.var(ddof=0),\"\\n\")\n",
    "print(\"standard deviation\\n\",my_data.std(ddof=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide by n - 2 in variance and standard derviation formulae.\n",
    "print(\"variance\\n\",my_data.var(ddof=2),\"\\n\")\n",
    "print(\"standard deviation\\n\",my_data.std(ddof=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the mean, we can easily extract a floating point number from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data[\"A\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.A.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "    <b>Optional Home Activity</b>: Calculate the standard deviation (using $n-1$) for the particulate matter example. Store the results in the dictionary <tt>ans_14b_2</tt> with keys \"low\" and \"high\". Hint: You'll need to calculate the standard deviation as a float and then save the answers into the dictionary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "ans_14b_2 = {\"low\":low.PM.std(), \"high\":high.PM.std()}\n",
    "print(ans_14b_2)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "14b-2",
     "locked": true,
     "points": "0.2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "import numpy as np\n",
    "assert np.abs(ans_14b_2[\"low\"] - 2.558) < 1E-2, \"Standard deviation for low altitude is not correct.\"\n",
    "assert np.abs(ans_14b_2[\"high\"] - 4.519) < 1E-2, \"Standard deviation for high altitude is not correct.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "<b>Optional Home Activity</b>: Show the following formulae are equivalent to the definitions for sample variance and sample standard deviation given above. <b>This is excellent practice for the next exam.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$s^2 = \\frac{1}{n-1} \\left( \\sum_{i=1}^{n} X_i^2 - n \\bar{X} \\right)$$\n",
    "\n",
    "$$s = \\sqrt{\\frac{1}{n-1} \\left( \\sum_{i=1}^{n} X_i^2 - n \\bar{X} \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample median also measures the center of a data set. To compute the median, we order the data from smallest to largest and find the middle. For a data set with a even number of objects, we average the two middle dataum.\n",
    "\n",
    "As you likely expect, pandas computes the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "    <b>Optional Home Activity</b>: Calculate the median for the particulate matter example. Store the results in the dictionary <tt>ans_14b_3</tt> with keys \"low\" and \"high\". Hint: You'll need to calculate the median as a float and then save the answers into the dictionary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "ans_14b_3 = {\"low\":low.PM.median(), \"high\":high.PM.median()}\n",
    "print(ans_14b_3)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "14b-3",
     "locked": true,
     "points": "0.2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "import numpy as np\n",
    "assert np.abs(ans_14b_3[\"low\"] - 3.18) < 1E-2, \"Median for low altitude is not correct.\"\n",
    "assert np.abs(ans_14b_3[\"high\"] - 5.75) < 1E-2, \"Median for high altitude is not correct.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Mode and Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sample mode** is the most common element in a sample.\n",
    "\n",
    "Let's review samples A and B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, these two data sets have the same mean and median but different variances. Let's compute the mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. In data set A, both there are two 0 elements and two 10 elements. Thus 0 and 10 are tied for the mode, and pandas returns two values. Likewise, 4 and 6 are tied for the mode in data set B.\n",
    "\n",
    "Let's look at the particulate matter example together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.PM.mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that both 1.11 and 1.63 are tied for the mode. Notice that `.mode()` returns a dictionary-like structure. We can access the first mode with the index (key) 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.PM.mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the second mode with index (key) 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.PM.mode()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate further using the `.value_counts()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.PM.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the number of times each value repeats in the data set, sorted from most to least common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "    <b>Optional Home Activity</b>: Calculate the mode for the high elevation particulate matter example. Store the results in the float <tt>ans_14b_4</tt>. Hint: You'll need to either extract the float from the pandas output with key 0 or manually enter your answer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "14b-4",
     "locked": false,
     "points": "0.2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "ans_14b_4 = high.PM.mode()[0]\n",
    "print(ans_14b_4)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "14b-4",
     "locked": true,
     "points": "0.2",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "import numpy as np\n",
    "assert np.abs(ans_14b_4 - 6.32) < 1E-2, \"Mode for high altitude is not correct.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sample range** is the different between the smallest and largest values in a sample. Pandas allows us to easily calculate these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data set\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify smallest values\n",
    "my_data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify largest values\n",
    "my_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quartiles and Percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median is the 50%-ile of the data set. Half of the elements are above and half are below. But we can generalize this to any numeric value between 0% and 100%. You'll notice that the pandas documentation says **quantile** instead of **percentile**. These terms mean the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 50% quantile, a.k.a, 50%-ile\n",
    "low.PM.quantile(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify this is the median\n",
    "low.PM.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quartiles** divide the data set into quarters (four pieces). Quartiles are specific percentiles:\n",
    "\n",
    "| Quartiles | Percentile / Quantile |\n",
    "| - | - |\n",
    "| 1st | 25% |\n",
    "| 2nd | 50% |\n",
    "| 3rd | 75% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the low altitude particulate matter example again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of entries\n",
    "low.PM.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains 138 datum. So what happens if we want to compute the 52.1%-ile?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas will give us a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low.PM.quantile(0.521)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the answer ends in ...000004. *What is going on?*\n",
    "\n",
    "Because there are only 138 elements, the quantiles increment by 0.724...%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/138"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is doing interpolation under the hood because there is not a datum exactly at the 52.1%-ile. The ...000004 ending is an artifact of inexact floating point arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "    <b>Optional Home Activity</b>: Calculate the 80%-ile for the high elevation particulate matter example. Store the results in the float <tt>ans_14b_5</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "ans_14b_5 = high.PM.quantile(0.8)\n",
    "print(ans_14b_5)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "14b-5",
     "locked": true,
     "points": "0.1",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "import numpy as np\n",
    "assert np.abs(ans_14b_5 - 8.822) < 1E-2, \"Quantile not correct. Make sure you are computing for high altitude and 80%-ile.\"\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Data Example: Exam 1 Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `Exam-1-scores.csv` contains anonymized numeric scores from Exam 1 this semester. Let's open the file with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Open the file\n",
    "exam1_full = pd.read_csv(\"Exam_1_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print entire dataframe\n",
    "print(exam1_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see 65 rows (students) and 20 columns. A perfect score was 60 points. Let's loop over the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in exam1_full.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a new Pandas dataframe with only the problem totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create empty dictionary\n",
    "new_data = {}\n",
    "new_data['Total'] = exam1_full['Total Score']\n",
    "new_data['P1'] = exam1_full['1-A'] + exam1_full['1-B']\n",
    "new_data['P2'] = exam1_full['2-A-1'] + exam1_full['2-A-2'] + exam1_full['2-B']\n",
    "new_data['P2'] += exam1_full['2-C-1'] + exam1_full['2-C-2'] + exam1_full['2-C-3']\n",
    "new_data['P3'] = exam1_full['3-A'] + exam1_full['3-B-1'] + exam1_full['3-B-2'] + exam1_full['3-B-3']\n",
    "new_data['P4'] = exam1_full['4-A-1'] + exam1_full['4-A-2'] + exam1_full['4-B']\n",
    "new_data['P4'] += exam1_full['4-B'] + exam1_full['4-C-1'] + exam1_full['4-C-2']\n",
    "new_data['P5'] = exam1_full['5-A'] + exam1_full['5-B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "exam1 = pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: Print the first and last five elements of the data set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22736,
     "status": "ok",
     "timestamp": 1551970243340,
     "user": {
      "displayName": "Alexander Dowling",
      "photoUrl": "https://lh3.googleusercontent.com/-LChdQ2m5OQE/AAAAAAAAAAI/AAAAAAAAAA0/JeXJe4vQJ7M/s64/photo.jpg",
      "userId": "00988067626794866502"
     },
     "user_tz": 300
    },
    "id": "xF9R5s_o30Tf",
    "outputId": "c3c6077a-3147-465f-dbd7-3f4b50cee4b2"
   },
   "outputs": [],
   "source": [
    "# print top (\"head\") of data frame\n",
    "exam1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print bottom (\"tail\") of data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: Compute the mean, median, mode, standard deviation, and quartiles using pandas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25%-ile\n",
    "q25 = exam1.quantile(0.25)\n",
    "print(q25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 50%-ile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 75%-ile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas offers a single function to compute these summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all in one line\n",
    "exam1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Covariance\n",
    "\n",
    "Standard devation (and variance) measure the average squared distance from each element of a dataset and the mean. Standard deviation is for a single dimension.\n",
    "\n",
    "Often, we want to know to the extent two variables are related. Let $X_1$, $X_2$, ..., $X_n$ and $Y_1$, $Y_2$, ..., $Y_n$ be the sample. Each pair ($X_i$, $Y_i$) corresponds to one experiment. For example, $X$ could be the effluent temperature and $Y$ could be the conversion for an adiabitic reactor. The experiment was repeated for $n$ trials.\n",
    "\n",
    "\n",
    "The **sample covariance** if a generalization of variance from one to two dimensions:\n",
    "\n",
    "$$s_{X,Y}^2 = \\frac{1}{n-1} \\sum_{i}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})$$\n",
    "\n",
    "$\\bar{X}$ and $\\bar{Y}$ are the sample means for variables $X$ and $Y$.\n",
    "\n",
    "If both $X_i$ and $Y_i$ tend to move together, i.e., both are either above ($Y_i > \\bar{Y}$ when $X_{i} > \\bar{X}$) or below ($Y_i < \\bar{Y}$ when $X_{i} < \\bar{X}$) their sample means, then $s_{X,Y}^2 >> 0$.\n",
    "\n",
    "If they move in oppositive directions, i.e., $Y_i > \\bar{Y}$ when $X_{i} < \\bar{X}$ and $Y_i > \\bar{Y}$ when $X_{i} < \\bar{X}$, then $s_{X,Y}^2 << 0$.\n",
    "\n",
    "If there is not a strong trend, then $s_{X,Y}^2 \\approx 0$.\n",
    "\n",
    "We can quickly compute covariance with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam1.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: What are the units of sample covariance?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Home Activity Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, it is more convinent to interpret **sample correlation**, which is sample covariance scaled by the standard deviation of each variable.\n",
    "\n",
    "$$\n",
    "r_{X,Y} = \\frac{s_{X,Y}}{s_X \\cdot s_Y}\n",
    "$$\n",
    "\n",
    "Recall, $s_{X,Y}$ is the covariance. $s_{X}$ and $s_{Y}$ are the standard deviations for variable $X$ and $Y$.\n",
    "\n",
    "By construction, correlation is bounded between -1 and 1. During the remainded of the semester, we will see why the following rules hold:\n",
    "\n",
    "| Correlation | Interpretation |\n",
    "| - | - |\n",
    "| $r_{X,Y} = -1.0$ | **Perfect negative correlation.** Samples $X_i$ and $Y_i$ lie exactly on a straight line with a negative slope. |\n",
    "| $-1.0 < r_{X,Y} < 0$ | **Negative correlation.** If we fit a line to the data $X_i$ and $Y_i$, the slope would be negative.\n",
    "| $r_{X,Y} = 0$ | **No correlation.** If we fit a line to the data $X_i$ and $Y_i$, the slope would be zero. |\n",
    "| $0 < r_{X,Y} < 1$ | **Positive correlation.** If we fit a line to the data $X_i$ and $Y_i$, the slope would be positive.\n",
    "| $r_{X,Y} = -1.0$ | **Perfect positive correlation.** Samples $X_i$ and $Y_i$ lie exactly on a straight line with a positive slope. |\n",
    "\n",
    "(Sorry for the weird table wrapping.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: What are the units of sample correlation?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correlation for the exam1 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam1.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: Interpret correlation for the exam data. Why is the diagonal exactly 1.0. What would a negative correlation mean?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we first normalize the scores by the number of available points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame\n",
    "exam1_norm = exam1.copy()\n",
    "\n",
    "# Divide by the total number of available points\n",
    "exam1_norm[\"P1\"] = exam1_norm[\"P1\"] / 8\n",
    "exam1_norm[\"P2\"] = exam1_norm[\"P2\"] / 14\n",
    "exam1_norm[\"P3\"] = exam1_norm[\"P3\"] / 10\n",
    "exam1_norm[\"P4\"] = exam1_norm[\"P4\"] / 17\n",
    "exam1_norm[\"P5\"] = exam1_norm[\"P5\"] / 14\n",
    "exam1_norm[\"Total\"] = exam1_norm[\"Total\"] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix\n",
    "exam1_norm.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "exam1_norm.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: Why is the correlation matrix not changed by scaling?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics for Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables are qualititative. Recall our example from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here failure location was a categorical variable. We cannot compute the median, standard deviation, or range for a categorical variable. Instead, we often compute **frequencies** by counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['Failure Location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instead compute the **sample proportions** by normalizing (dividing by) the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['Failure Location'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics and Population Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample statistic we have discussed (e.g., mean, median, standard deviation, covariance) has a counterpart for the population. We will call numerical summaries of samples **statistics** and numerical summaries of populations **parameters**. A central idea in data analysis is to use statistics to estimate/infer parameters. Often, we cannot directly measure a population. So instead we sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14d. Visualizing Data\n",
    "\n",
    "**Further Reading**: ยง1.3 in Navidi (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to visualize data. This semester, we will focus on scatter plots, histograms, and boxplots. We will use matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already learned how to make a scatter plot in matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exam1.P2,exam1.P3,marker='.',linestyle=\" \")\n",
    "plt.xlabel(\"Score for P2 [points]\")\n",
    "plt.ylabel(\"Score for P3 [points]\")\n",
    "plt.title(\"Exam 1 Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: Why do these data appear to line on a grid?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Old Faithful\n",
    "\n",
    "To illustrate key concepts for plotting, we will explore Table 1.6 in Navidi (2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "old_faithful = pd.read_csv('table1-6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_faithful.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has two columns:\n",
    "1. The duration of dormant periods (in minutes).\n",
    "2. Whether the previous erruption was *short* or *long*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "    <b>Home Activity</b>: Take a look at the nicely formatted table in the textbook (pg. 32).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of dormant periods using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(old_faithful.Dormancy)\n",
    "plt.xlabel(\"Dormancy (in minutes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Old Faithful (Table 1.6)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal axis of the histogram shows the continous variable of interest. The vertical axis shows the number of observations in each bin. By default, `matplotlib.pylot` uses equal size bins and automatically determines the number of bins. We can, of course, override these options.\n",
    "\n",
    "https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "\n",
    "https://matplotlib.org/3.1.0/gallery/statistics/hist.html\n",
    "\n",
    "For example, let's use 11 bins that go from 40 to 95 minutes to match the textbook. Let's also make the bins 50% transparent (`alpha=0.5`). Finally, we will set `density=True` to normalize by the number of observations and set the color to purple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(old_faithful.Dormancy, bins=11, range=(40, 95),alpha=0.5,density=True,color='purple')\n",
    "plt.xlabel(\"Dormancy (in minutes)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Old Faithful (Table 1.6)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is **bimodal**. There is one mode between 50 and 55 minutes and a second mode between 80 and 85 minutes. Often, bimodal data are due to not accounting for an important scientific phenomena.\n",
    "\n",
    "Let's split our data based on duration of the previous eruption. We will make two histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the elements where Eruption is 'Short'\n",
    "i = old_faithful.Eruption == 'Short'\n",
    "\n",
    "# Create the histogram for short\n",
    "plt.hist(old_faithful.Dormancy[i], bins=11, range=(40, 95),alpha=0.5,density=True,color='red',label='Short')\n",
    "\n",
    "# find the elements where Eruption is 'Long'\n",
    "j = old_faithful.Eruption == 'Long'\n",
    "\n",
    "# Create histogram for long\n",
    "plt.hist(old_faithful.Dormancy[j], bins=11, range=(40, 95),alpha=0.5,density=True,color='blue',label='Short')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel(\"Dormancy (in minutes)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Old Faithful (Table 1.6)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah ha, we can explain the two modes by accounting for the duration of the previous eruption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,0,255,0.05) ; padding: 10px; border: 1px solid darkblue;\"> \n",
    "<b>Class Activity</b>: Make a histogram of the low altitude emissions data set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# hint: you can access the data with \n",
    "# low.PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **boxplots** visualizes the median, first and third quartiles, and any outliers. Here is the basic anatomy of a boxplot:\n",
    "\n",
    "![boxplot](https://miro.medium.com/max/18000/1*2c21SkzJMf3frPXPAR_gZA.png)\n",
    "\n",
    "The **interquartile range** is the difference between the third and first quartiles. Recall, between the first and third quartiles are the middle 50% of the data. Any observations 1.5 times the IQR beyond the first and third quartiles are considers **extreme outliers** and marked with a $*$ or circle on the boxplot.\n",
    "\n",
    "Let's make a boxplot of the Old Faithful data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that both the data and labels are in lists,\n",
    "# and the lists have the same length\n",
    "plt.boxplot([old_faithful.Dormancy], labels=['All Data'])\n",
    "plt.ylabel(\"Dormancy (in minutes)\")\n",
    "plt.title(\"Old Faithful (Table 1.6)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The organge line is the median. There are no outliers shown on this boxplot.\n",
    "\n",
    "Now let's make a comparative boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the elements where Eruption is 'Short'\n",
    "i = old_faithful.Eruption == 'Short'\n",
    "\n",
    "# find the elements where Eruption is 'Long'\n",
    "j = old_faithful.Eruption == 'Long'\n",
    "\n",
    "# Create two side-by-side boxplots\n",
    "plt.boxplot([old_faithful.Dormancy[i], old_faithful.Dormancy[j]], labels=['Short','Long'])\n",
    "plt.ylabel(\"Dormancy (in minutes)\")\n",
    "plt.title(\"Old Faithful (Table 1.6)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see some outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgba(0,255,0,0.05) ; padding: 10px; border: 1px solid darkgreen;\"> \n",
    "<b>Optional Home Activity</b>: After class, test your skills by making a comparative boxplot for the low and high altitude emssions data set. Compare your work to Figure 1.15 in Navidi.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Practice: Exam Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative distribution function\n",
    "n = np.arange(0,len(exam1_norm))/len(exam1_norm)\n",
    "plt.plot(exam1_norm['Total'].sort_values()*100,n,color=\"black\",label=\"Total\",linewidth=2)\n",
    "plt.plot(exam1_norm['P1'].sort_values()*100,n,color=\"red\",label=\"P1\",linewidth=2,linestyle=\"--\")\n",
    "plt.plot(exam1_norm['P2'].sort_values()*100,n,color=\"blue\",label=\"P2\",linewidth=2,linestyle=\"--\")\n",
    "plt.plot(exam1_norm['P3'].sort_values()*100,n,color=\"green\",label=\"P3\",linewidth=2,linestyle=\"--\")\n",
    "plt.plot(exam1_norm['P4'].sort_values()*100,n,color=\"purple\",label=\"P4\",linewidth=2,linestyle=\"--\")\n",
    "plt.plot(exam1_norm['P5'].sort_values()*100,n,color=\"orange\",label=\"P5\",linewidth=2,linestyle=\"--\")\n",
    "plt.xlabel(\"Score [%]\")\n",
    "plt.ylabel(\"Fraction of Class\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "fig, axes = plt.subplots(nrows=6, ncols=1, figsize=(10,20))\n",
    "ax0, ax1, ax2, ax3, ax4, ax5 = axes.flatten()\n",
    "\n",
    "\n",
    "ax0.hist(exam1_norm['Total'].sort_values()*100,color=\"black\",label=\"Total\")\n",
    "ax0.set_title('Total')\n",
    "\n",
    "ax1.hist(exam1_norm['P1'].sort_values()*100,color=\"red\",label=\"P1\")\n",
    "ax1.set_title('Problem 1')\n",
    "\n",
    "ax2.hist(exam1_norm['P2'].sort_values()*100,color=\"blue\",label=\"P2\")\n",
    "ax2.set_title(\"Problem 2\")\n",
    "\n",
    "ax3.hist(exam1_norm['P3'].sort_values()*100,color=\"green\",label=\"P3\")\n",
    "ax3.set_title('Problem 3')\n",
    "\n",
    "ax4.hist(exam1_norm['P4'].sort_values()*100,color=\"purple\",label=\"P4\")\n",
    "ax4.set_title('Problem 4')\n",
    "\n",
    "ax5.hist(exam1_norm['P5'].sort_values()*100,color=\"orange\",label=\"P5\")\n",
    "ax5.set_title('Problem 5')\n",
    "\n",
    "plt.xlabel(\"Score [%]\")\n",
    "plt.ylabel(\"Fraction of Class\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Boxplot\n",
    "plt.boxplot(exam1_norm.transpose(), labels=exam1_norm.columns)\n",
    "plt.ylabel(\"Score [%]\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "L14-Descriptive-Statistics.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
