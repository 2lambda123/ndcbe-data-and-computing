{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Theoretical Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before Class**:\n",
    "* Read Chapter 6 in Savov (2020) and take notes\n",
    "* Watch the following videos and take notes:\n",
    "  * [Eigenvectors and eigenvalues](https://www.3blue1brown.com/lessons/eigenvalues)\n",
    "  * [A quick trick for computing eigenvalues](https://www.3blue1brown.com/lessons/quick-eigen)\n",
    "  * [Abstract vector spaces](https://www.3blue1brown.com/lessons/abstract-vector-spaces)\n",
    "* Compile a list of questions to bring to class\n",
    "\n",
    "**During and After Class**:\n",
    "* Take notes (on paper or a tablet computer)\n",
    "* Complete this notebook, submit you answer via Gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # If running on colab, install ipympl\n",
    "    !pip install -q ipympl\n",
    "\n",
    "    # Enable 3rd party output/widgets\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "# % means a \"magic command\" that is special\n",
    "# for Jupyter notebooks.\n",
    "# This specific command enables interactive plots\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are our standard imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the eigendecomposition of\n",
    "\n",
    "$$\n",
    "{\\bf A}_0 = \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 3 & 0 \\\\ 2 & -4 & 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This example is from pg. 302 - 303 of Savov (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by writing the definition of an eigenvalue:\n",
    "\n",
    "$$\n",
    "{\\bf A}_0 {\\bf w}_i = \\lambda_i {\\bf w}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left({\\bf A}_0 - \\lambda_i {\\bf I} \\right) {\\bf w}_i = {\\bf 0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{det}\\left({\\bf A}_0 - \\lambda {\\bf I} \\right) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{det}\\left( \\begin{bmatrix} 1 - \\lambda & 2 & 0 \\\\ 0 & 3 - \\lambda & 0 \\\\ 2 & -4 & 2 - \\lambda \\end{bmatrix}    \\right) = 0 \n",
    "$$\n",
    "\n",
    "Now let's compute the determinant:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\text{det}\\left( \\begin{bmatrix} 1 - \\lambda & 2 & 0 \\\\ 0 & 3 - \\lambda & 0 \\\\ 2 & -4 & 2 - \\lambda \\end{bmatrix}    \\right) \n",
    "&= (1 - \\lambda) \\text{det} \\left(\\begin{bmatrix} 3 - \\lambda & 0 \\\\ -4 & 2 - \\lambda \\end{bmatrix} \\right) \n",
    "+ 2 ~ \\text{det} \\left(\\begin{bmatrix} 0 & 0 \\\\ 2 & 2 - \\lambda \\end{bmatrix}\\right)\n",
    "+ 0  ~ \\text{det} \\left(\\begin{bmatrix} 0 & 3 - \\lambda \\\\ 2 & -4 \\end{bmatrix}\\right) \\\\\n",
    "&= (1-\\lambda) \\left( (3 - \\lambda)(2 - \\lambda) + 4 \\cdot 0 \\right) \n",
    "- 2 \\left(0 (2 - \\lambda) - 0 \\cdot 2 \\right)\n",
    "+ 0 \\left( -0 \\cdot 4 - 2 (3 - \\lambda) \\right) \\\\\n",
    "& = (1-\\lambda) (3 - \\lambda) (2 - \\lambda)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This gives the characteristic polynomial:\n",
    "\n",
    "$$\n",
    "\\text{det}\\left({\\bf A}_0 - \\lambda {\\bf I} \\right) = 0 = (1-\\lambda) (3 - \\lambda) (2 - \\lambda)\n",
    "$$\n",
    "\n",
    "By inspection, see the roots of the characteristic equation are $\\lambda = 3$, $\\lambda = 2$, and $\\lambda = 1$ (written from largest to smallest by convention). These are the eigenvalues. (Yes, this is textbook example, hence the math was easy.)\n",
    "\n",
    "Next, we need to calculate the eigenvector that corresponds to each eigenvalue. To do this, we need to calculate the null space of $({\\bf A}_0 - \\lambda_i {\\bf I})$ for each $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the null space, we need to convert ${\\bf A}_0 - \\lambda_i {\\bf I}$ into reduced row eschelon form. You can do this by hand of course, which would be excellent practice. We'll use `sympy` to speed up this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sympy import Matrix\n",
    "\n",
    "def rref_eigensystem(eigval=1):\n",
    "    ''' Calculate the reduced row eschelon form of\n",
    "    A0 - eigval * I\n",
    "    for this specific example, i.e., A0 is hard coded\n",
    "    into this function for simplicity\n",
    "\n",
    "    Argument:\n",
    "        eigval: eigenvalue\n",
    "\n",
    "    '''\n",
    "\n",
    "   \n",
    "    # Define A\n",
    "    A_ = Matrix([[1 - eigval, 2, 0], [0, 3 - eigval, 0], [2, -4, 2 - eigval]])\n",
    "    #print(\"A - eigval I=\\n\",A_)\n",
    "    # Calculate reduced row eschelon form\n",
    "    rref_ = A_ .rref()\n",
    "    print(\"rref =\")\n",
    "    print(rref_)\n",
    "\n",
    "rref_eigensystem(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to go from RREF to the null space. Let's use the RREF to write a system of linear equations:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 & 0 & \\frac{1}{2} \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "~\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ t \\end{bmatrix} = {\\bf 0}\n",
    "$$\n",
    "\n",
    "Notice in the RREF matrix, there is no pivot (1 on the diagonal) for the column corresponding for $x_3$. Thus, we replaced $x_3$ with $t$ to denote it is a free variable. From this linear system, we obtain:\n",
    "\n",
    "$$\n",
    "x_1 + \\frac{t}{2} = 0, \\qquad x_2 = 0, \\qquad 0 = 0\n",
    "$$\n",
    "\n",
    "With a little algebra, we obtain:\n",
    "\n",
    "$$\n",
    "x_1 = \\frac{-t}{2}, \\quad x_2 = 0, \\quad x_3 = t\n",
    "$$\n",
    "\n",
    "Thus the solution to the linear system is:\n",
    "\n",
    "$$\n",
    "{\\bf x} = \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\end{bmatrix} t, \\quad \\forall t \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\text{null}({\\bf A}_0 - 1 {\\bf I}) = \\text{span}\\left( \\begin{bmatrix} -1 \\\\ 0 \\\\ 2 \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "Let's verify this with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_eigen(eigval=1):\n",
    "    ''' Calculate the null space\n",
    "    A0 - eigval * I\n",
    "    for this specific example, i.e., A0 is hard coded\n",
    "    into this function for simplicity\n",
    "\n",
    "    Argument:\n",
    "        eigval: eigenvalue\n",
    "\n",
    "    '''\n",
    "    \n",
    "    A_ = np.array([[1 - eigval, 2, 0], [0, 3 - eigval, 0], [2, -4, 2 - eigval]])\n",
    "\n",
    "    print(\"null space =\\n\", la.null_space(A_))\n",
    "\n",
    "null_eigen(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to out by hand answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3 = np.array([-1, 0, 2])\n",
    "print(\"eigenvector 3 (unscaled) =\", w3)\n",
    "\n",
    "w3 = w3 / la.norm(w3)\n",
    "\n",
    "print(\"eigenvector 3 (scaled) =\", w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As homework, calculate the remaining two eigenvectors on paper and then verify with Python. In other words, repeat the procedure above. You may use Python to calculate the RREFs, but you should calculate the null space with paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the first eigenvector here (\\lambda = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the second eigenvector here (\\lambda = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Eigendecompositions\n",
    "\n",
    "Calculate the eigendecomposition of the following matrices by hand and then verify with Python. Please complete the entire eigendecomposition calculation on paper, including find the eigenvectors (RREF, null space). This is practice for the quiz.\n",
    "\n",
    "$$\n",
    "{\\bf A}_1 = \\begin{bmatrix} 2 & 0 \\\\ 0 & -3 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eigen(A):\n",
    "    ''' Calculate eigendecomposition and print information\n",
    "\n",
    "    '''\n",
    "\n",
    "    print(\"A =\\n\",A)\n",
    "    w, l = la.eig(A)\n",
    "    print(\"eigenvalues = \",w)\n",
    "    print(\"eigenvectors =\\n\", l)\n",
    "    print(\"rank =\", np.linalg.matrix_rank(A))\n",
    "    print(\"condition number =\", np.linalg.cond(A))\n",
    "\n",
    "A = np.array([[2, 0], [0, -3]])\n",
    "print_eigen(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\bf A}_2 = \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = np.array([[1, 0], [1, 0]])\n",
    "print_eigen(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\bf A}_3 = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = np.array([[2, 1], [0, 2]])\n",
    "print_eigen(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to compute\n",
    "\n",
    "$$\n",
    "{\\bf A}^n = {\\bf A A} ...  {\\bf A }\n",
    "$$\n",
    "\n",
    "This requires $n$ matrix multiplications, which is tedious.\n",
    "\n",
    "Recall, we decompose a diagonalizable matrix into its eigenvectors and eigenvalues.\n",
    "\n",
    "$$\n",
    "{\\bf A} = {\\bf Q} {\\bf \\Lambda} {\\bf Q}^{-1}\n",
    "$$\n",
    "\n",
    "Here ${\\bf Q}$ is the matrix of right eigenvectors and ${\\bf \\Lambda} = \\text{diag}(\\lambda_1, \\lambda_2, ...)$. If $\\bf A$ is symmetric, then ${\\bf Q}^T$ = ${\\bf Q}^{-1}$.\n",
    "\n",
    "Now let's plug this into the matrix exponential:\n",
    "\n",
    "$$\n",
    "{\\bf A}^n = \\left({\\bf Q} {\\bf \\Lambda} {\\bf Q}^{-1} \\right)^n = \\left({\\bf Q} {\\bf \\Lambda} {\\bf Q}^{-1} \\right) \\left({\\bf Q} {\\bf \\Lambda} {\\bf Q}^{-1} \\right) ... \\left({\\bf Q} {\\bf \\Lambda} {\\bf Q}^{-1} \\right)\n",
    "$$\n",
    "\n",
    "Finally, we can simplify as ${\\bf Q}^{-1} {\\bf Q} = {\\bf I}$\n",
    "\n",
    "$$\n",
    "{\\bf A}^n = {\\bf Q} {\\bf \\Lambda}^n {\\bf Q}^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easy to compute because ${\\bf \\Lambda}$ is a diagonal matrix, thus:\n",
    "\n",
    "$$\n",
    "{\\bf \\Lambda}^n = \\begin{bmatrix} (\\lambda_1)^n & 0 & \\dots \\\\ 0 & (\\lambda_2)^n & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate\n",
    "\n",
    "$$\n",
    "\\left( {\\bf A}_3 \\right)^3 = \\left( \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix} \\right)^3\n",
    "$$\n",
    "\n",
    "with pencil and paper two ways: simple matrix multiplication and with the eigendecomposition approach. (Hint, you already calculcated the eigendecomposition.) Then, verify your calculation with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "A3_cubed = A3 @ A3 @ A3\n",
    "\n",
    "print(\"A3 cubed = \\n\", A3_cubed)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranformations as Projections onto the Eigenbasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall, we can represent a linear transformation $T({\\bf x})$ as the matrix vector product $\\bf A x$. Let's interpret the linear transformation $\\bf A$ using an eigendecomposition. This requires ${\\bf A}$ to be a [diagonalizable matrix](https://en.wikipedia.org/wiki/Diagonalizable_matrix).\n",
    "\n",
    "$$\n",
    "\\bf{A} = \\sum_i \\lambda_i {\\bf w}_i {\\bf w}_i^T\n",
    "$$\n",
    "\n",
    "Here, $\\lambda_i$ are the eigenvalues and ${\\bf w}_i$ are the corresponding eigenvectors.\n",
    "\n",
    "$$\n",
    "\\bf{A x} = \\sum_i \\lambda_i \\left( {\\bf w}_i {\\bf w}_i^T {\\bf x} \\right)\n",
    "$$\n",
    "\n",
    "Also recall that dot products are communitive, i.e., ${\\bf w}_i^T {\\bf x} = {\\bf x}^T {\\bf w}_i = {\\bf x} \\cdot {\\bf w}_i$.\n",
    "\n",
    "$$\n",
    "\\bf{A x} = \\sum_i \\lambda_i \\left( {\\bf w}_i {\\bf x} \\cdot {\\bf w}_i  \\right)\n",
    "$$\n",
    "\n",
    "Recall that ${\\bf w}_i$ are unit vectors, i.e., $||{\\bf w}_i|| = 1$ and rearrange:\n",
    "\n",
    "$$\n",
    "\\bf{A x} = \\sum_i \\lambda_i \\left( \\frac{{\\bf x} \\cdot {\\bf w}_i}{|| {\\bf w}_i ||^2}  {\\bf w}_i  \\right)\n",
    "$$\n",
    "\n",
    "This means we can intrepet the matrix multiplication $\\bf{A x}$ as projecting ${\\bf x}$ onto each eigenvector ${\\bf w}_i$ and scaling by the eigenvalue $\\lambda_i$\n",
    "\n",
    "$$\n",
    "\\bf{A x} = \\sum_i \\lambda_i \\left( \\Pi_{u} {\\bf x} \\right)\n",
    "$$\n",
    "\n",
    "Let's visualize this in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Plot vectors\n",
    "def plot_vector(vector, color, label, p = [0,0,0], linestyle=\"-\"):\n",
    "    ''' Plot a 3D vector\n",
    "        Arguments:\n",
    "            vector: np array\n",
    "            color: 'r', 'b', 'g', etc.\n",
    "            label: string for legend\n",
    "            p: point to start vector\n",
    "        \n",
    "    '''\n",
    "    assert len(vector) == 3, \"Must be a 3D vector\"\n",
    "    assert len(p) == 3, \"p must have length 3\"\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.quiver(p[0], p[1], p[2], vector[0], vector[1], vector[2], color=color, arrow_length_ratio=0.1, label=label, linestyle=linestyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear transformation\n",
    "A = np.array([[1,0,-0.5], [0, 1, 0], [-0.5, 1, 0.5]])\n",
    "print(\"A\\n=\",A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's verify ${\\bf A}$ is [normal](https://en.wikipedia.org/wiki/Normal_matrix), i.e., ${\\bf{A}}^T{\\bf A} = {\\bf A}{\\bf{A}}^T$.  Recall, a normal matrix is also diagonalizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A.T@A=\\n\",A.T@A)\n",
    "print(\"A@A.T=\\n\",A.T@A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 1, 1]\n",
    "\n",
    "print(\"A =\\n\",A)\n",
    "print(\"x =\", x)\n",
    "print(\"Ax =\", A@x)\n",
    "\n",
    "# Calculate eigendecomposition\n",
    "l, w = la.eig(A)\n",
    "print(\"eigenvalues = \",l)\n",
    "print(\"eigenvectors =\\n\", w)\n",
    "\n",
    "# Create empty plot\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "\n",
    "colors = ['blue','red','green']\n",
    "\n",
    "plot_vector(x, 'black', r'${\\bf x}$')\n",
    "\n",
    "# Plot eigenvectors\n",
    "for i in range(3):\n",
    "    plot_vector(w[:,i],colors[i],r'${\\bf w}_'+str(i+1)+'$')\n",
    "\n",
    "# Plot projections\n",
    "for i in range(3):\n",
    "    proj = np.dot(x, w[:,i]) * w[:,i] / np.linalg.norm(w[:,i])**2\n",
    "    #plot_vector(proj, 'black', r'$\\lambda_'+str(i+1)+'\\Pi_{w_'+str(i+1)+'}} {x}$')\n",
    "    #plot_vector(proj, 'black', r'$\\Pi_{w_'+str(i+1)+'}} x$', linestyle='-.')\n",
    "    plot_vector(proj, 'black', r'$\\mathrm{proj.}~{\\bf x}~\\mathrm{onto}~{\\bf w}_{'+str(i)+'}$', linestyle='-.')\n",
    "\n",
    "plot_vector(A @ x, 'purple', r'${\\bf A x}$')\n",
    "\n",
    "# Set labels and limits\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_xlim([-2,2])\n",
    "ax.set_ylim([-2,2])\n",
    "ax.set_zlim([-2,2])\n",
    "\n",
    "plt.legend(ncol=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD and Matrix Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [](../03/chapter5.ipynb) for a quick introduction to SVD and its relationship to condition number.\n",
    "\n",
    "How do we calculate the SVD of a matrix?\n",
    "\n",
    "The left signular vectors are the eigenvectors of ${\\bf A}{\\bf A}^T$.\n",
    "\n",
    "The right singular vectors are the eigenvectors of ${\\bf A}^T{\\bf A}$.\n",
    "\n",
    "The eigenvalues $\\lambda_i$ of ${\\bf A}{\\bf A}^T$ and ${\\bf A}^T{\\bf A}$ are equal. Moreover, the singular values $\\sigma_i = \\sqrt{\\lambda_i}$.\n",
    "\n",
    "Thus, the linear tranformation $T_{A}({\\bf A}) = {\\bf A x}$ can be interpreted as a three step process:\n",
    "1. Convert ${\\bf x}$ from the standard basis to the basis of the right singular vectors\n",
    "2. Scale componenets by the singular values\n",
    "3. Convert the output from the basis of the left singular vectors to the standard basis\n",
    "\n",
    "Finally, we can write a matrix using SVD as follows:\n",
    "\n",
    "$$\n",
    "{\\bf A} = {\\bf U \\Sigma V}^T = \\sum_{i} \\sigma_i {\\bf u}_i {\\bf v}^{T}_i\n",
    "$$\n",
    "\n",
    "Thus, we can compress ${\\bf A}$ by truncated the summation. Let's see this with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the interactive plots\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Import opencv\n",
    "import cv2\n",
    "  \n",
    "# Import a picture of Prof. Dowling in grayscale\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Download\n",
    "    !wget 'https://dowlinglab.nd.edu/assets/320333/dowling_alex_sq.jpg'\n",
    "    image_location = 'dowling_alex_sq.jpg'\n",
    "else:\n",
    "    # Use local copy already in repo (no internet required)\n",
    "    image_location = '../../media/dowling_alex_sq.jpg'\n",
    "\n",
    "img = cv2.imread(image_location, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img,title=None):\n",
    "    ''' Plot grayscale image stored as a matrix\n",
    "    '''\n",
    "\n",
    "    plt.matshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.title(title)\n",
    "\n",
    "show_image(img,'Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigendecomposition\n",
    "# l, w = la.eig(img)\n",
    "\n",
    "U, S, Vh = la.svd(img, full_matrices=True)\n",
    "\n",
    "plt.semilogy(np.abs(S))\n",
    "plt.xlabel(\"Index\",fontsize=18)\n",
    "plt.ylabel(r\"$|~\\sigma_i~|$\",fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_image(original_image,n=10):\n",
    "    ''' Compress imagage\n",
    "\n",
    "    Arguments:\n",
    "        original_image: matrix with values represented greyscale colors\n",
    "        n: number of eigenvalues to keep\n",
    "\n",
    "    Actions:\n",
    "        Show compressed image\n",
    "\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Singular value decomposition\n",
    "    U, S, Vh = la.svd(original_image, full_matrices=True)\n",
    "\n",
    "\n",
    "    # Compress matrix\n",
    "    compressed = U[:,0:n] @ np.diag(S[0:n]) @ Vh[0:n,:]\n",
    "\n",
    "    # Plot image\n",
    "    show_image(compressed ,'n='+str(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only 10 largest singular values (and corresponding vectors)\n",
    "compress_image(img, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only 20 largest singular values (and corresponding vectors)\n",
    "compress_image(img, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only 100 largest singular values (and corresponding vectors)\n",
    "compress_image(img, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD to Diagnose Linearly Dependent Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A4 = np.array([[1, 2, 0], [0, 1, 1], [1, 0, -2]])\n",
    "print(\"A4=\\n\",A4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the row 2 equals (row 0) minus 2 times (row 1). In other words, this system of equations is rank deficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(A4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = la.svd(A4, full_matrices=True)\n",
    "\n",
    "print(\"left singular vectors\\nU=\\n\",U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"singular values\\nS=\\n\",S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"right singular vectors (transposed)\\nVh=\\n\",Vh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret left singular vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the left singular vector corresponding to the near-zero singular value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original unit vector\n",
    "print(\"unit vector\",U[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale by the inverse of the lower left corner of U\n",
    "print(\"scaled vector\",U[:,-1] / U[0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that 1 times (row 0) plus -2 times (row 1) plus -1 times (row 2) equals 0. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1*A4[0,:] - 2*A4[1,:] - 1*A4[2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take Away**: The left singular vector corresponding to a near-zero singular value tells us how to add the rows of a matrix together to achieve a rank deficiency. Thus, SVD helps us identify the linearly dependent equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret right singular vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the right singular vector corresponding to the near-zero singular value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original unit vector\n",
    "print(\"unit vector\",Vh[-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale by the inverse of the lower right corner of Vh\n",
    "print(\"scaled vector\",Vh[-1,:] / Vh[-1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that 2 times (columun 0) plus -1 times (column 1) plus 1 times (column 2) equals 0. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*A4[:,0] - 1*A4[:,1] + 1*A4[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take Away**: The right singular vector corresponding to a near-zero singular value tells us how to add the columns of a matrix together to achieve a rank deficiency. SVD is a very help tool to find modeling mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Create your own example below to show how SVD helps diagnose a linearly dependent system of equations. Specifically:\n",
    "1. Propose a linearly dependent system of equations (as an $\\bf A$ matrix).\n",
    "2. Perform SVD\n",
    "3. Interpret the left and right singular vectors corresponding to the near-zero singular value.\n",
    "\n",
    "Everyone should submit a unique $\\bf A$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and print A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SVD and print components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale left singular vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation Here**: [replace with 1 sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale right singular vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Interpretation Here**: [replace with 1 sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
